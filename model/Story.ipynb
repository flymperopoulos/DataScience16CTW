{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://www.onedropnirvana.com/wp-content/uploads/2014/07/airbnb.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Your Airbnb Pricing\n",
    "#### Patrick Huston & Filippos Lymperopoulos | Spring 2016\n",
    "\n",
    "Whether an apartment for a night, a castle for a week, or a villa for a month, Airbnb connects people to unique travel experiences, at any price point, in more than 34,000 cities and 190 countries. And with world-class customer service and a growing community of users, Airbnb is the easiest way for people to monetize their extra space and showcase it to an audience of millions. However, how does one make sure that they price their property realistically to attract guests? How do hosts make sure that they adjust their listing's price with respect to the neighborhood it is in, the number of beds, type of rooms and beds the property is offering and other such parameters? \n",
    "\n",
    "An approach to the afore-mentioned questions was developed as a 2-week project by Patrick Huston and Filippos Lymperopoulos for the [Data Science](https://sites.google.com/site/datascience16/) class at Olin College of Engineering, Spring 2016.\n",
    "\n",
    "------\n",
    "\n",
    "## Introduction\n",
    "This notebook aims to document our process in creating an ML pipeline to predict Airbnb listing prices given an input set of features. A major focus of this exploration is to write modular, well-designed components that can easily be used in different modeling situations. At the same time, we are using the model developed in this notebook to provide the backend infrastructure of a web interface that will serve as a guide for potential hosts to price their listings.\n",
    "\n",
    "## Dataset\n",
    "We were able to gain access to Airbnb data via [Inside Airbnb](http://insideairbnb.com/get-the-data.html). This resource provided us with information on listings across different cities, *listings.csv*, as well as the relevant dates of availability of the listings presented in the listings datasheet, as given in *calendar.csv*. We ended up not utilizing the *calendar.csv* data, as we focused our exploration more on the features that affect price on different listings on the service.\n",
    "\n",
    "## Libraries\n",
    "To run our scripts, we need to import a series of standard libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import binaryHelper as be\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the Listings Data\n",
    "We loaded the different listings for individual cities and concatenated them into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "listingsBoston = pd.read_csv('../data/listingsBoston.csv')\n",
    "listingsSF = pd.read_csv('../data/listingsSF.csv')\n",
    "listingsLA = pd.read_csv('../data/listingsLA.csv')\n",
    "listingsDC = pd.read_csv('../data/listingsDC.csv')\n",
    "listingsSeattle = pd.read_csv('../data/listingsSeattle.csv')\n",
    "\n",
    "# List of city dataframes\n",
    "frames = [listingsBoston, listingsSF, listingsLA, listingsDC, listingsSeattle]\n",
    "\n",
    "# Dataframe with all listings\n",
    "listingsAll = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "\n",
    "The Airbnb dataset is composed of both numerical features - data like the number of beds, the number of bedrooms, and the number of bathrooms - and categorical features - data like the neighborhood, the type of room, the type of property listed and the type of beds offered. \n",
    "\n",
    "Both types of features are important in predicting the price of a given listing. While the numerical features can be used directly, we'll have to apply some additional processing and encoding steps to get the categorical data in a representation that can be used as a feature in our models.\n",
    "\n",
    "### Numerical Features\n",
    "\n",
    "The numerical features we plan on using directly are the following:\n",
    "- `bedrooms` - The number of bedrooms included in the listing\n",
    "- `beds` - The number of beds included in the listing\n",
    "- `bathrooms` - The number of bathrooms included in the listing\n",
    "- `accommodates` - The number of people the listing accommodates\n",
    "\n",
    "Other non-categorical features we'll be extracting are:\n",
    "- `num_amenities` - Using the 'amenities' feature, we can extract the number of amenities offered\n",
    "- `days_host` - Using the 'host_since' feature, we can compute the number of days the host has been a host, potentially a measure of experience\n",
    "- `price` - We must clean the 'price' feature to extract a numerical (rather than string) value for the price of the listing\n",
    "\n",
    "### Categorical Features\n",
    "\n",
    "There are several pertinent categorical features that intuitively seem to have great significance in the price of the listing. To use them in a model, however, we'll need to take some steps to encode them numerically. Listed here are the features we'll be using in the model:\n",
    "\n",
    "- `neighbourhood` - The neighbourhood the property is in\n",
    "- `property_type` - The type of property (e.g. apartment, bed and breakfast, house)\n",
    "- `room_type` - The type of room (e.g. Shared room, Entire home/apt, or Private room\n",
    "- `bed_type` - The type of bed offered (e.g. Real Bed, Futon, Couch, Pull-out Sofa, Airbed\n",
    "\n",
    "\n",
    "### Encoding Methodologies\n",
    "There are several options for encoding categorical features. \n",
    "\n",
    "##### Ordinal Encoding\n",
    "\n",
    "In ordinal encoding, each categorical value takes on an integer value. Ordinal encoding doesn't add extra columns in the feature matrix, which can dilute the other features included. The major drawback of an ordinal encoding is that it inserts a notion of a relationship between each category and the dependent variable (price, in this case). In some situations, this may be appropriate (e.g. in the bed_type feature, there is a natural ordering/relationship between the bed_type and the price of the listing). In all ordinal mappings we have assumed a contant increment value between different categories.\n",
    "\n",
    "##### One-Hot (Dummy) Encoding\n",
    "\n",
    "In a one-hot encoding scheme, each category is represented as its own new feature that takes on a binary value - 1 if the input fits into a given category and zero, otherwise. For a category with *n* levels, this means adding *n* new columns to our feature matrix. For low values of *n*, this may be okay, but higher values of *n* risk the possibiilty of blowing the dimensionality of the feature matrix way out of proportion. This can be hazardous, as features of length much less than *n* will not get to affect the prediction as much.\n",
    "\n",
    "##### Binary Encoding\n",
    "\n",
    "Binary encoding is a cool alternative to one-hot encoding for the representation of categorical values. First, each value takes on an integer value in an ordinal encoding. From this, each value may be represented as a binary number. Finally, this binary number is split up into individual bits, and each is inserted into the feature matrix as a new column. A big advantage of this method compared to the One-Hot Encoding one is that the number of the resulting features column is significantly less and, hence, addresses the dimensionality issue we alluded on previously.\n",
    "\n",
    "### Encoding Choices\n",
    "\n",
    "Armed with a wealth of information on categorical encoding, we made the following decisions for each of the categorical features in the dataset.\n",
    "\n",
    "#### Neighbourhood\n",
    "Due to its high dimensionality (420 possibilities), we chose a binary encoding to represent the neighbourhood feature. An ordinal encoding might also be possible, but in research we've done, a binary encoding almost always seemed to ourperform ordinal encodings. We ended up reducing the \"represented\" number of features from 420 to 9. \n",
    "\n",
    "#### Property Type\n",
    "Property was another category that had multiple dimensions (26 options), hence we decided to process the categorical data by encoding them with a binary method. We initially attempted sum encoding however that did not yield improved results. We ended up reducing the \"represented\" number of features from 26 to 5. \n",
    "\n",
    "#### Room Type & Bed Type\n",
    "Observing the results from the previous encoding procedures we decided to perform a different form of encoding, where we utilized a combination of one-hot and binary encoding. This method significantly aided our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinal Mapping Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As afore-mentioned, we decided to map all categorical features to ordinal values. We used our prior knowledge to \"rank\" the ordinality for the room_type mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Method for lienar ordinal encoding\n",
    "def create_ordinal_mapping(data, parameter):\n",
    "    parameter_mapping = {}\n",
    "    \n",
    "    for index, parameter in enumerate(sorted(data[parameter].unique())):\n",
    "        parameter_mapping[parameter] = index\n",
    "    \n",
    "    return parameter_mapping\n",
    "\n",
    "# Ordinal mappings on neighbourhood, bed, room and property type\n",
    "neighbourhood_mapping = create_ordinal_mapping(listingsAll, \"neighbourhood_cleansed\")\n",
    "bed_type_mapping = create_ordinal_mapping(listingsAll, \"bed_type\")\n",
    "room_type_mapping = create_ordinal_mapping(listingsAll, \"room_type\")\n",
    "property_type_mapping = create_ordinal_mapping(listingsAll, \"property_type\")\n",
    "\n",
    "# Sorting based on \"prior knowledge\" room quality\n",
    "shared_room_ord = room_type_mapping[\"Shared room\"] \n",
    "entire_apt_ord = room_type_mapping[\"Entire home/apt\"]\n",
    "room_type_mapping[\"Shared room\"] = entire_apt_ord\n",
    "room_type_mapping[\"Entire home/apt\"] = shared_room_ord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing \n",
    "\n",
    "To facilitate the process of cleaning, we defined a cleaning Helper and Processor Class that took care of constructing  the appropriate fields in the dataframe used for our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper\n",
    "\n",
    "The class `Helper` exposes a set of helpful functions that each deal with processing an individual feature in the dataset. For example, `amenities_to_list` takes in an individual row from the `amenities` column - represented as a string - and converts it into a more useful list format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Helper():\n",
    "    \n",
    "    ''' \n",
    "    Helper exposes a set of functions aimed processing individual \n",
    "    features in the dataset.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, data_attrs):\n",
    "        self.data_attrs = data_attrs\n",
    "    \n",
    "    def amenities_to_list(self, amenities):\n",
    "        \"\"\"Converts string representation of amenities list\"\"\"\n",
    "        amenities = amenities.replace('{', '')\n",
    "        amenities = amenities.replace('}', '')\n",
    "        amenities = amenities.replace('\\\"', '')\n",
    "        return amenities.split(',')\n",
    "\n",
    "    def num_amenities(self, amenitiesList):\n",
    "        \"\"\"Creates new feature out of the number of amenities\"\"\"\n",
    "        return len(amenitiesList)\n",
    " \n",
    "    def price_to_int(self, price):\n",
    "        \"\"\"Converts string representation of price to float\"\"\"\n",
    "        price = price.replace(',', '')\n",
    "        price = price.replace('$', '')\n",
    "        return float(price)\n",
    "    \n",
    "    def date_to_days(self, startDay):\n",
    "        \"\"\"Returns the number of days an individual has been a host\"\"\"\n",
    "        dStart = datetime.strptime(startDay, \"%Y-%m-%d\")\n",
    "        dEnd = datetime.strptime(self.data_attrs['date_min'], \"%Y-%m-%d\")\n",
    "        return abs((dEnd - dStart).days)\n",
    "    \n",
    "    def row_to_ordinal(self, row, mapping):\n",
    "        \"\"\"Returns mapped ordinal values\"\"\"\n",
    "        return self.data_attrs[mapping][row]\n",
    "\n",
    "helper = Helper({'date_min': listingsAll[listingsAll.host_since.isnull() == False].host_since.max(), \n",
    "                 'neighbourhood_mapping': neighbourhood_mapping, \n",
    "                 'bed_type_mapping':bed_type_mapping, \n",
    "                 'room_type_mapping':room_type_mapping,\n",
    "                 'property_type_mapping':property_type_mapping\n",
    "                });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Processor\n",
    "The class `cleanProcessor` handles two things - it applies the methods defined in `Helper` to its dataset and performs some additional processing. Null-fitting is an important process handled, while encoding applications and concatenation of the final dataframe is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cleanProcessor():\n",
    "\n",
    "    ''' \n",
    "    cleanProcessor exposes a set of functions aimed at cleaning up the dataframe\n",
    "    in question and addressing null exceptions and mappings.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def clean_listings(self):\n",
    "        \"\"\" \n",
    "        Returns clean dataframe after null values are removed and appropriate encoding\n",
    "        are applied\n",
    "        \"\"\"\n",
    "        df_clean = self.data.copy()\n",
    "        \n",
    "        # Null-fitting \n",
    "        df_clean.loc[df_clean.review_scores_rating.isnull(), 'review_scores_rating'] = 90\n",
    "        df_clean.loc[df_clean.host_since.isnull(), 'host_since'] = '2015-10-02'\n",
    "        df_clean.loc[df_clean.bedrooms.isnull(), 'bedrooms'] = 0\n",
    "        df_clean.loc[df_clean.bathrooms.isnull(), 'bathrooms'] = 0\n",
    "        df_clean.loc[df_clean.beds.isnull(), 'beds'] = 0\n",
    "        df_clean.loc[df_clean.property_type.isnull(), 'property_type'] = \"Other\"\n",
    "        \n",
    "        # Preprocessing data using Helper instance\n",
    "        df_clean['amenities'] = df_clean['amenities'].apply(helper.amenities_to_list)\n",
    "        df_clean['num_amenities'] = df_clean['amenities'].apply(helper.num_amenities)\n",
    "        df_clean['price'] = df_clean['price'].apply(helper.price_to_int)\n",
    "        df_clean['days_host'] = df_clean['host_since'].apply(helper.date_to_days)\n",
    "\n",
    "        # Mapping implementations for neighbourhood and property\n",
    "        df_clean['neighbourhood_binary'] = df_clean['neighbourhood_cleansed'].apply(helper.row_to_ordinal, args=(\"neighbourhood_mapping\",))\n",
    "        df_clean['property_binary_encoded'] = df_clean['property_type'].apply(helper.row_to_ordinal, args=(\"property_type_mapping\",))\n",
    "        \n",
    "        # Binary encoding for neighborhoods\n",
    "        encoder = be.BinaryEncoder(cols=['neighbourhood_binary'])\n",
    "        binary_neighbourhoods = encoder.transform(df_clean)\n",
    "        \n",
    "        # Binary encoding for property_type\n",
    "        property_encoder = be.BinaryEncoder(cols=['property_binary_encoded'])\n",
    "        binary_properties = property_encoder.transform(df_clean)\n",
    "        \n",
    "        # One-hot and Ordinal Encoding for bed_type and room_type\n",
    "        bed_type = pd.get_dummies(df_clean.bed_type)    \n",
    "        df_clean[\"bed_type\"] = df_clean[\"bed_type\"].apply(helper.row_to_ordinal, args=(\"bed_type_mapping\",))\n",
    "        room_type = pd.get_dummies(df_clean.room_type)    \n",
    "        df_clean[\"room_type\"] = df_clean[\"room_type\"].apply(helper.row_to_ordinal, args=(\"room_type_mapping\",))\n",
    "\n",
    "        # One-hot encoding for cancellation policy and renaming of resulting columns\n",
    "        cancellation_policy = pd.get_dummies(df_clean.cancellation_policy)\n",
    "        cancellation_policy.rename(columns={'strict':'cancellation_strict'})\n",
    "        cancellation_policy.rename(columns={'flexible':'cancellation_flexible'})\n",
    "        cancellation_policy.rename(columns={'moderate':'cancellation_moderate'})\n",
    "                \n",
    "        data = pd.concat([df_clean, cancellation_policy, binary_neighbourhoods, bed_type, room_type, binary_properties], axis=1)\n",
    "\n",
    "        return data\n",
    "\n",
    "clean_processor_listings = cleanProcessor(listingsAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use our clean_processor instance to clean our listings data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listingsClean = clean_processor_listings.clean_listings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Models\n",
    "\n",
    "Now that we've done a good amount of initial preprocessing on our data, let's move towards some predictive modeling. Our goal is to develop a model that will predict a price given listing parameters. We'll experiment with including different features, and see which combination fo features and models produces the best results.\n",
    "\n",
    "After some initial research we've decided to start off by trying four different models:\n",
    "\n",
    "1. Linear Lasso\n",
    "       The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent.\n",
    "       \n",
    "2. ElasticNet \n",
    "       ElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\n",
    "       \n",
    "3. Support Vector Regression\n",
    "       Support Vector Regression is an implementation of regression that uses the SVM approach. In this case, we'll be using the linear kernel.\n",
    "            \n",
    "4. Ridge Regression\n",
    "       Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares.\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear Lasso\n",
    "lasso = linear_model.Lasso(alpha = .01)\n",
    "\n",
    "# ElasticNet\n",
    "elasticNet = linear_model.ElasticNet(alpha = 0.1, l1_ratio=0.7)\n",
    "\n",
    "# Ridge Regression\n",
    "ridgeRegression = linear_model.Ridge(alpha = .5)\n",
    "\n",
    "# Support Vector Regression\n",
    "svr = svm.SVR(C=1.0, epsilon=0.2)\n",
    "\n",
    "# Dictionary of Models\n",
    "models = {'lasso': lasso, 'elasticNet': elasticNet, 'ridgeRegression': ridgeRegression }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Models\n",
    "\n",
    "Now, let's define `ModelHelper`, a class that will facilitate the process of testing our models.\n",
    "\n",
    "#### Methodology for Testing\n",
    "\n",
    "To test our model, we'll be using the `cross_val_score` method provided by `scikit-learn`. Additionally, we'll be using `cross_val_score` coupled with a StratifiedKFold train-test splitting step. This step ensures that each neighborhood will have equal representation in both the training and test datasets. This is necessary because the data is sorted by neighbourhood. By default, a train-test split would likely give us training and test sets where a given neighbourhood only appears in one or the other - essentially nullifying the predictive power of the neighbourhood as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelHelper():\n",
    "    \n",
    "    ''' \n",
    "    ModelHelper exposes a set of functions aimed at facilitating the dataset\n",
    "    manipulation (train-test splitting) and model testing process\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, X, y, features):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.XFeat = X[features]\n",
    "    \n",
    "    def cross_validate(self, model, cv=3):\n",
    "        '''Cross-validates model within trainnig set with a split of 'cv' - default value of 3'''\n",
    "        cv = StratifiedKFold(self.X.neighbourhood_cleansed)\n",
    "        return cross_val_score(model, self.XFeat, self.y, cv=cv).mean()\n",
    "\n",
    "    def train_test_splitter(self, model, train_size=0.6, save=False):\n",
    "        '''Performs train-test split on data, trains on train, tests on test, returns score, model, data'''\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.XFeat, self.y, train_size=train_size)\n",
    "        model.fit(X_train, y_train)\n",
    "        return X_train, X_test, y_train, y_test, model\n",
    "\n",
    "    def test_models(self, models):\n",
    "        '''Iterates over all different models and print out their results of train_test_splitter'''\n",
    "        for modelName, model in models.iteritems():\n",
    "            print '{} : {}'.format(modelName, self.cross_validate(model))\n",
    "    \n",
    "    def save_model(self, model, filename=\"model.pkl\"):\n",
    "        '''Persists model to disk for later use in backend API'''\n",
    "        model.fit(self.XFeat, self.y)\n",
    "        joblib.dump(model, filename) \n",
    "\n",
    "    def save_mappings(self, filename=\"newmapping.pkl\"):\n",
    "        '''Persists model to disk for later use in backend API'''\n",
    "        property_type_mapping_copy = property_type_mapping.copy()\n",
    "        room_type_mapping_copy = room_type_mapping.copy()\n",
    "        bed_type_mapping.update(property_type_mapping_copy)\n",
    "        bed_type_mapping.update(room_type_mapping_copy)        \n",
    "        bed_type_mapping.update(neighbourhood_mapping)\n",
    "        \n",
    "        pickle.dump(bed_type_mapping, open(filename, 'wb')) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customizing feature columns\n",
    "As noted above, we performed appropriate encoding regimes to the potential input features of our model. A binary encoding transformation on the property_type coluns generates 4 columns indexed columns that we then pass into our `allFeatures` list, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary_encoded_properties = [\"property_binary_encoded_{}\".format(i) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['bedrooms', 'beds', 'bathrooms', 'accommodates',\n",
    "            'neighbourhood_binary_0', 'neighbourhood_binary_1', \n",
    "            'neighbourhood_binary_2', 'neighbourhood_binary_3',\n",
    "            'neighbourhood_binary_4', 'neighbourhood_binary_5',\n",
    "            'neighbourhood_binary_6', 'neighbourhood_binary_7',\n",
    "            'neighbourhood_binary_8', 'bed_type', 'Airbed', \n",
    "            'Couch', 'Futon', 'Pull-out Sofa', 'Real Bed', \n",
    "            'room_type','Shared room','Private room', 'Entire home/apt']\n",
    "\n",
    "# All features added\n",
    "allFeatures = features + binary_encoded_properties\n",
    "\n",
    "# Dump features to pickle file for later use\n",
    "pickle.dump(allFeatures, open( '../app/utils/modelFeatures.pkl', 'wb' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling our `ModelHelper` class we can test multiple models and save a specific one for future usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticNet : 0.274001414485\n",
      "ridgeRegression : 0.274696027367\n",
      "lasso : 0.274703106196\n"
     ]
    }
   ],
   "source": [
    "# Model instance with clean listings, price and features\n",
    "mHelper = ModelHelper(listingsClean, listingsClean.price, allFeatures)\n",
    "\n",
    "# Test multiple models\n",
    "mHelper.test_models(models)\n",
    "\n",
    "# Pick to save lasso model representation\n",
    "mHelper.save_model(lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Results Analysis\n",
    "We tested a series of different models and as we can see from the results above, the models are producing similar outputs. All such models belong in the category of regression models. We decided to extract the score from those tests which is given as the coefficient of determination $R^{2}$, R-squared, of the prediction. R-squared is a statistical measure of how close the data are to the fitted regression line.\n",
    "\n",
    "In essence what this output is providing us with, is a coefficient of determination for the price of a given listing. Our `Lasso Model` iteration does a descent job, as it manages to explain to a significant extent the variability of the response data around the mean. The closer the model output is to unity, the higher the R-squared value is and the model explains *all* the variability present. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Node Integration\n",
    "\n",
    "The final component of our project was that of actually predicting a price for a given listing. We implemented a NodeJS backend that exposes the prediction power of our model as an API. A frontend ReactJS application with D3.js maps interacts with this API to create a visual price prediction tool for Airbnb.\n",
    "\n",
    "For instance, a prediction example is discussed below. The input matrix, `sample`, represents a listing with all features properly encoded numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'lasso.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-777a41d2886f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlassomodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lasso.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0mwas\u001b[0m \u001b[0msaved\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marrays\u001b[0m \u001b[0mcannot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mmemmaped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \"\"\"\n\u001b[0;32m--> 446\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_handle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;31m# We are careful to open the file handle early and keep it open to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;31m# avoid race-conditions on renames. That said, if data are stored in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'lasso.pkl'"
     ]
    }
   ],
   "source": [
    "lassomodel = joblib.load('lasso.pkl')\n",
    "\n",
    "sample = np.array([2, 2, 2, 2, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 1])\n",
    "sample = sample.reshape(1,-1)\n",
    "\n",
    "lassoload.predict(sample)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection and Future Work\n",
    "\n",
    "\n",
    "This project was a real eye-opener to the world of Data Science and Vizualization. The former field allowed us to explore routes pertaining to preprocessing, feature engineering and model development of our system, while the latter, combining full-stack development libraries with powerful vizualization tools, gave us the chance to see an integrated user-friendly final product come to life. \n",
    "\n",
    "As shown in multiple iPython Notebooks under the model_exploration directory, when we started off with this project, we tried to investigate correlations between different categories and price. This helped us a lot initially understanding how such relationships are formed and aided us in the further developement of a model with a given set of carefuly feature engineered categories.\n",
    "\n",
    "Certainly a way to improve our model would be to include more features and investigate different ways of encoding certain categorical features, while playing around more with parameters affecting the behavior or models could certainly be worth investigating int the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
