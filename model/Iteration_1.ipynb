{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airbnb Price Prediction - Optimizing Listings\n",
    "#### Patrick Huston & Filippos Lymperopoulos | Spring 2016\n",
    "\n",
    "*This notebook aims to document our process in creating a ML pipeline to predict Airbnb listing prices given an input set of features. A major focus of this exploration is to write modular, well-designed components that could easily be taken and applied to a different modeling situation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "0. What should we do with space?\n",
    "1. Explore encoding for bed_type\n",
    "2. Explore encoding for room_type\n",
    "3. Explore encoding for property_type\n",
    "4. Explore encoding for amenities\n",
    "5. Explore log-loss + understand accuracy meaning (what is .28? is that good?)\n",
    "6. Document our code and decisions we've made\n",
    "    - CleanProcessor: filling nulls, binary encoding, representing features\n",
    "    - Explain our decisions in feature engineering and why we did them\n",
    "    - Modeling decisions\n",
    "7. Nulls in room_type? Get them from the space description\n",
    "8. Is there anything interesting in the name of the listing?\n",
    "\n",
    "\n",
    "### NOTES\n",
    "\n",
    "1. Mean absolute deviation\n",
    "2. Log transform the price - how far away from the order of magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import binaryHelper as be\n",
    "import sumHelper as se\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "Airbnb provides an expansive listings dataset for public use. It includes all listings for major cities around the world. For the purpose of our exploration, we'll start off by using well-known US cities - Boston, San Francisco, Los Angeles, Washington DC, and Seattle. For each listing, Airbnb provides a large amount of features - check [here](https://github.com/flymperopoulos/DataScience16CTW/blob/master/report/listings_features.md) to see them all listed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Listings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "listingsBoston = pd.read_csv('../data/listingsBoston.csv')\n",
    "listingsSF = pd.read_csv('../data/listingsSF.csv')\n",
    "listingsLA = pd.read_csv('../data/listingsLA.csv')\n",
    "listingsDC = pd.read_csv('../data/listingsDC.csv')\n",
    "listingsSeattle = pd.read_csv('../data/listingsSeattle.csv')\n",
    "\n",
    "frames = [listingsBoston, listingsSF, listingsLA, listingsDC, listingsSeattle]\n",
    "\n",
    "listingsAll = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Calendar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calendarBoston = pd.read_csv('../data/calendarBoston.csv')\n",
    "calendarSF = pd.read_csv('../data/calendarSF.csv')\n",
    "calendarLA = pd.read_csv('../data/calendarLA.csv')\n",
    "calendarDC = pd.read_csv('../data/calendarDC.csv')\n",
    "calendarSeattle = pd.read_csv('../data/calendarSeattle.csv')\n",
    "\n",
    "frames = [calendarBoston, calendarSF, calendarLA, calendarDC, calendarSeattle]\n",
    "\n",
    "calendarAll = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Convenience\n",
    "\n",
    "To facilitate the process of cleaning, we've defined a cleaning helper and cleaning processor class below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{nan: 0, 'Tipi': 21, 'Lighthouse': 15, 'Apartment': 1, 'Earth House': 11, 'Other': 17, 'Cabin': 5, 'Tent': 20, 'Chalet': 8, 'Loft': 16, 'Yurt': 26, 'Train': 23, 'Treehouse': 24, 'Hut': 13, 'Plane': 19, 'Dorm': 10, 'Camper/RV': 6, 'Bed & Breakfast': 2, 'Island': 14, 'Bungalow': 4, 'Castle': 7, 'Parking Space': 18, 'Villa': 25, 'House': 12, 'Condominium': 9, 'Townhouse': 22, 'Boat': 3}\n"
     ]
    }
   ],
   "source": [
    "def create_ordinal_mapping(data, parameter):\n",
    "    parameter_mapping = {}\n",
    "    \n",
    "    for index, parameter in enumerate(sorted(data[parameter].unique())):\n",
    "        parameter_mapping[parameter] = index\n",
    "    \n",
    "    return parameter_mapping\n",
    "\n",
    "neighbourhood_mapping = create_ordinal_mapping(listingsAll, \"neighbourhood_cleansed\")\n",
    "bed_type_mapping = create_ordinal_mapping(listingsAll, \"bed_type\")\n",
    "room_type_mapping = create_ordinal_mapping(listingsAll, \"room_type\")\n",
    "property_type_mapping = create_ordinal_mapping(listingsAll, \"property_type\")\n",
    "\n",
    "# Sorting based on room quality\n",
    "shared_room_ord = room_type_mapping[\"Shared room\"] \n",
    "entire_apt_ord = room_type_mapping[\"Entire home/apt\"]\n",
    "room_type_mapping[\"Shared room\"] = entire_apt_ord\n",
    "room_type_mapping[\"Entire home/apt\"] = shared_room_ord\n",
    "\n",
    "print property_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper\n",
    "\n",
    "The class `Helper` exposes a set of helpful functions that each deal with processing an individual feature in the dataset. For example, `amenities_to_list` takes in an individual row from the `amenities` column - represented as a string - and converts it into a more useful list format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Helper():\n",
    "    def __init__(self, data_attrs):\n",
    "        self.data_attrs = data_attrs\n",
    "    \n",
    "    # Converts string representation of amenities list \n",
    "    def amenities_to_list(self, amenities):\n",
    "        amenities = amenities.replace('{', '')\n",
    "        amenities = amenities.replace('}', '')\n",
    "        amenities = amenities.replace('\\\"', '')\n",
    "        return amenities.split(',')\n",
    "\n",
    "    # Creates new feature out of the number of amenities\n",
    "    def num_amenities(self, amenitiesList):\n",
    "        return len(amenitiesList)\n",
    "\n",
    "    # Converts string representation of price to float \n",
    "    def price_to_int(self, price):\n",
    "        price = price.replace(',', '')\n",
    "        price = price.replace('$', '')\n",
    "        return float(price)\n",
    "    \n",
    "    def date_to_days(self, startDay):\n",
    "        dStart = datetime.strptime(startDay, \"%Y-%m-%d\")\n",
    "        dEnd = datetime.strptime(self.data_attrs['date_min'], \"%Y-%m-%d\")\n",
    "        return abs((dEnd - dStart).days)\n",
    "    \n",
    "    def row_to_ordinal(self, row, mapping):\n",
    "        return self.data_attrs[mapping][row]\n",
    "\n",
    "    \n",
    "helper = Helper({'date_min': listingsAll[listingsAll.host_since.isnull() == False].host_since.max(), \n",
    "                 'neighbourhood_mapping': neighbourhood_mapping, \n",
    "                 'bed_type_mapping':bed_type_mapping, \n",
    "                 'room_type_mapping':room_type_mapping,\n",
    "                 'property_type_mapping':property_type_mapping\n",
    "                });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Processor\n",
    "The class `cleanProcessor` does handles two things - it applies the methods defined in `Helper` to its dataset and performs some additional processing like null-filling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cleanProcessor():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def clean_listings(self):\n",
    "        df_clean = self.data.copy()\n",
    "        \n",
    "        # TODO: What? - Better techniques for filling nulls\n",
    "        df_clean.loc[df_clean.review_scores_rating.isnull(), 'review_scores_rating'] = 90\n",
    "        df_clean.loc[df_clean.host_since.isnull(), 'host_since'] = '2015-10-02'\n",
    "        df_clean.loc[df_clean.bedrooms.isnull(), 'bedrooms'] = 0\n",
    "        df_clean.loc[df_clean.bathrooms.isnull(), 'bathrooms'] = 0\n",
    "        df_clean.loc[df_clean.beds.isnull(), 'beds'] = 0\n",
    "        df_clean.loc[df_clean.property_type.isnull(), 'property_type'] = \"Other\"\n",
    "        \n",
    "        df_clean['amenities'] = df_clean['amenities'].apply(helper.amenities_to_list)\n",
    "        df_clean['num_amenities'] = df_clean['amenities'].apply(helper.num_amenities)\n",
    "        df_clean['price'] = df_clean['price'].apply(helper.price_to_int)\n",
    "        df_clean['days_host'] = df_clean['host_since'].apply(helper.date_to_days)\n",
    "        df_clean['neighbourhood_binary'] = df_clean['neighbourhood_cleansed'].apply(helper.row_to_ordinal, args=(\"neighbourhood_mapping\",))\n",
    "        df_clean['property_sum_encoded'] = df_clean['property_type'].apply(helper.row_to_ordinal, args=(\"property_type_mapping\",))\n",
    "        \n",
    "        # Binary encoding for neighborhoods\n",
    "        encoder = be.BinaryEncoder(cols=['neighbourhood_binary'])\n",
    "        binary_neighbourhoods = encoder.transform(df_clean)\n",
    "        \n",
    "        # Sum encoding for property type\n",
    "        sum_encoder = be.BinaryEncoder(cols=['property_sum_encoded'])\n",
    "        sum_properties = sum_encoder.transform(df_clean)\n",
    "        \n",
    "        # One-hot and Ordinal Encoding for bed_type and room_type\n",
    "        bed_type = pd.get_dummies(df_clean.bed_type)    \n",
    "        df_clean[\"bed_type\"] = df_clean[\"bed_type\"].apply(helper.row_to_ordinal, args=(\"bed_type_mapping\",))\n",
    "        room_type = pd.get_dummies(df_clean.room_type)    \n",
    "        df_clean[\"room_type\"] = df_clean[\"room_type\"].apply(helper.row_to_ordinal, args=(\"room_type_mapping\",))\n",
    "\n",
    "        # One-hot encoding for cancellation policy\n",
    "        cancellation_policy = pd.get_dummies(df_clean.cancellation_policy)\n",
    "        cancellation_policy.rename(columns={'strict':'cancellation_strict'})\n",
    "        cancellation_policy.rename(columns={'flexible':'cancellation_flexible'})\n",
    "        cancellation_policy.rename(columns={'moderate':'cancellation_moderate'})\n",
    "                \n",
    "        data = pd.concat([df_clean, cancellation_policy, binary_neighbourhoods, bed_type, room_type], axis=1)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def clean_calendar(self):\n",
    "        self.df_clean = df.copy()\n",
    "        self.df_clean = self.df_clean[self.df_clean.available == 't']\n",
    "        self.df_clean['price'] = self.df_clean['price'].apply(helper.price_to_int)\n",
    "        return self.df_clean\n",
    "\n",
    "clean_processor_listings = cleanProcessor(listingsAll)\n",
    "clean_processor_calendar = cleanProcessor(calendarAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listingsClean = clean_processor_listings.clean_listings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Private -- 80.0 \n",
      "Shared -- 47.0 \n",
      "Entire -- 159.0 \n"
     ]
    }
   ],
   "source": [
    "print '{} -- {} '.format('Private', listingsClean[listingsClean.room_type == 1].price.median()) \n",
    "print '{} -- {} '.format('Shared', listingsClean[listingsClean.room_type == 0].price.median()) \n",
    "print '{} -- {} '.format('Entire', listingsClean[listingsClean.room_type == 2].price.median()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Real Bed': 4, 'Futon': 2, 'Couch': 1, 'Pull-out Sofa': 3, 'Airbed': 0}\n"
     ]
    }
   ],
   "source": [
    "print bed_type_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real -- 120.0 \n",
      "Pull-out -- 79.5 \n",
      "Futon -- 70.0 \n",
      "Air -- 65.0 \n",
      "Couch -- 55.0 \n"
     ]
    }
   ],
   "source": [
    "print '{} -- {} '.format('Real', listingsClean[listingsClean.bed_type == 4].price.median())\n",
    "print '{} -- {} '.format('Pull-out', listingsClean[listingsClean.bed_type == 3].price.median()) \n",
    "print '{} -- {} '.format('Futon', listingsClean[listingsClean.bed_type == 2].price.median()) \n",
    "print '{} -- {} '.format('Air', listingsClean[listingsClean.bed_type == 0].price.median()) \n",
    "print '{} -- {} '.format('Couch', listingsClean[listingsClean.bed_type == 1].price.median()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Models\n",
    "\n",
    "Now that we've done a good amount of initial preprocessing on our data, let's move towards some predictive modeling. Our goal is to develop a model that will predict a price given listing parameters. We'll experiment with including different features, and see which combination fo features and models produces the best results.\n",
    "\n",
    "After some initial research we've decided to start off by trying four different models - \n",
    "\n",
    "1. Linear Lasso\n",
    "       The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent.\n",
    "       \n",
    "2. ElasticNet \n",
    "       ElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\n",
    "       \n",
    "3. Support Vector Regression\n",
    "       Support Vector Regression is an implementation of regression that uses the SVM approach. In this case, we'll be using the linear kernel.\n",
    "            \n",
    "4. Ridge Regression\n",
    "       Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares.\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear Lasso\n",
    "lasso = linear_model.Lasso(alpha = 0.1)\n",
    "\n",
    "# ElasticNet\n",
    "elasticNet = linear_model.ElasticNet(alpha = 0.1, l1_ratio=0.7)\n",
    "\n",
    "# Ridge Regression\n",
    "ridgeRegression = linear_model.Ridge(alpha = .5)\n",
    "\n",
    "# Support Vector Regression\n",
    "svr = svm.SVR(C=1.0, epsilon=0.2)\n",
    "\n",
    "models = {'lasso': lasso, 'elasticNet': elasticNet, 'ridgeRegression': ridgeRegression }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Models\n",
    "\n",
    "Now, let's define `ModelHelper`, a class that will facilitate the process of testing our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelHelper():\n",
    "    ''' \n",
    "    ModelHelper exposes a set of functions aimed at facilitating the dataset\n",
    "    manipulation (train-test splitting) and model testing process\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, X, y, features):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.XFeat = X[features]\n",
    "    \n",
    "    def cross_validate(self, model, cv=3):\n",
    "        '''Cross-validates model within trainnig set with a split of 'cv' - default value of 3'''\n",
    "        cv = StratifiedKFold(self.X.neighbourhood_cleansed)\n",
    "        return cross_val_score(model, self.XFeat, self.y, cv=cv).mean()\n",
    "\n",
    "    def train_test_splitter(self, model, train_size=0.6, save=False):\n",
    "        '''Performs train-test split on data, trains on train, tests on test, returns score, model, data'''\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.XFeat, self.y, train_size=train_size)\n",
    "        model.fit(X_train, y_train)\n",
    "        return X_train, X_test, y_train, y_test, model\n",
    "\n",
    "    def test_models(self, models):\n",
    "        '''Iterates over all different models and print out their results of train_test_splitter'''\n",
    "        for modelName, model in models.iteritems():\n",
    "            print '{} : {}'.format(modelName, self.cross_validate(model))\n",
    "    \n",
    "    def save_model(self, fitted_model, filename=\"model.pkl\"):\n",
    "        '''Persists model to disk for later use in backend API'''\n",
    "        joblib.dump(fitted_model, filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum_encoded_list= []\n",
    "for i in range(26):\n",
    "    sum_encoded_list.append(\"property_sum_encoded_{}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: add property_type to features --> encoding\n",
    "features = ['days_host', 'num_amenities', 'bedrooms', 'beds',\n",
    "            'neighbourhood_binary_0', 'neighbourhood_binary_1', \n",
    "            'neighbourhood_binary_2', 'neighbourhood_binary_3',\n",
    "            'neighbourhood_binary_4', 'neighbourhood_binary_5',\n",
    "            'neighbourhood_binary_6', 'neighbourhood_binary_7',\n",
    "            'neighbourhood_binary_8', 'bathrooms', 'accommodates',\n",
    "            'bed_type', 'Airbed', 'Couch', 'Futon', 'Pull-out Sofa',\n",
    "            'Real Bed', 'room_type', 'Entire home/apt', 'Private room',\n",
    "            'Shared room']\n",
    "\n",
    "allFeatures = features + sum_encoded_list\n",
    "\n",
    "\n",
    "pickle.dump(allFeatures, open( '../app/utils/modelFeatures.pkl', 'wb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticNet : 0.273435882183\n",
      "ridgeRegression : 0.273776643271\n",
      "lasso : 0.273765463073\n"
     ]
    }
   ],
   "source": [
    "mHelper = ModelHelper(listingsClean, listingsClean.price, features)\n",
    "\n",
    "mHelper.test_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['House', 'Apartment', 'Villa', 'Condominium', 'Other',\n",
       "       'Bed & Breakfast', 'Loft', 'Townhouse', 'Boat', 'Castle', 'Dorm',\n",
       "       nan, 'Bungalow', 'Cabin', 'Tent', 'Island', 'Camper/RV', 'Plane',\n",
       "       'Yurt', 'Treehouse', 'Chalet', 'Hut', 'Earth House', 'Lighthouse',\n",
       "       'Parking Space', 'Tipi', 'Train'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listingsClean.property_type.unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
