{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airbnb Price Prediction - Optimizing Listings\n",
    "#### Patrick Huston & Filippos Lymperopoulos | Spring 2016\n",
    "\n",
    "*This notebook aims to document our process in creating a ML pipeline to predict Airbnb listing prices given an input set of features. A major focus of this exploration is to write modular, well-designed components that could easily be taken and applied to a different modeling situation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import binaryHelper as be\n",
    "import sumHelper as se\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "Airbnb provides an expansive listings dataset for public use. It includes all listings for major cities around the world. For the purpose of our exploration, we'll start off by using well-known US cities - Boston, San Francisco, Los Angeles, Washington DC, and Seattle. For each listing, Airbnb provides a large amount of features - check [here](https://github.com/flymperopoulos/DataScience16CTW/blob/master/report/listings_features.md) to see them all listed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Listings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "listingsBoston = pd.read_csv('../data/listingsBoston.csv')\n",
    "listingsSF = pd.read_csv('../data/listingsSF.csv')\n",
    "listingsLA = pd.read_csv('../data/listingsLA.csv')\n",
    "listingsDC = pd.read_csv('../data/listingsDC.csv')\n",
    "listingsSeattle = pd.read_csv('../data/listingsSeattle.csv')\n",
    "\n",
    "frames = [listingsBoston, listingsSF, listingsLA, listingsDC, listingsSeattle]\n",
    "\n",
    "listingsAll = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Calendar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calendarBoston = pd.read_csv('../data/calendarBoston.csv')\n",
    "calendarSF = pd.read_csv('../data/calendarSF.csv')\n",
    "calendarLA = pd.read_csv('../data/calendarLA.csv')\n",
    "calendarDC = pd.read_csv('../data/calendarDC.csv')\n",
    "calendarSeattle = pd.read_csv('../data/calendarSeattle.csv')\n",
    "\n",
    "frames = [calendarBoston, calendarSF, calendarLA, calendarDC, calendarSeattle]\n",
    "\n",
    "calendarAll = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Convenience\n",
    "\n",
    "To facilitate the process of cleaning, we've defined a cleaning helper and cleaning processor class below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering \n",
    "\n",
    "The Airbnb dataset is composed of both numerical features - data like the number of beds, the number of bedrooms, and the number of bathrooms - and categorical features - data like the neighborhood, the type of room, and the type of the bed. \n",
    "\n",
    "Both types of features will be important in predicting the price of a given listing. While the numerical features can be used directly, we'll have to take some additional processing and encoding steps to get the categorical data in a representation that can be used as a feature in our models.\n",
    "\n",
    "#### Numerical Features\n",
    "\n",
    "The numerical features we plan on using directly are the following:\n",
    "- `bedrooms` - The number of bedrooms included in the listing\n",
    "- `beds` - The number of beds included in the listing\n",
    "- `bathrooms` - The number of bathrooms included in the listing\n",
    "- `accommodates` - The number of people the listing accommodates\n",
    "\n",
    "Other non-categorical features we'll be extracting are:\n",
    "- `num_amenities` - Using the 'amenities' feature, we can extract the number of amenities offered\n",
    "- `days_host` - Using the 'host_since' feature, we can compute the number of days the host has been a host, potentially a measure of experience\n",
    "- `price` - We must clean the 'price' feature to extract a numerical (rather than string) value for the price of the listing\n",
    "\n",
    "#### Categorical Features\n",
    "\n",
    "There are several pertinent categorical features that intuitively seem to have great significance in the price of the listing. To use them in a model, however, we'll need to take some steps to encode them numerically. Listed here are the features we'll be using in the model:\n",
    "\n",
    "- `neighbourhood` - The neighbourhood the property is in\n",
    "- `property_type` - The type of property (e.g. apartment, bed and breakfast, house)\n",
    "- `room_type` - The type of room (e.g. Shared room, Entire home/apt, or Private room\n",
    "- `bed_type` - The type of bed offered (e.g. Real Bed, Futon, Couch, Pull-out Sofa, Airbed\n",
    "\n",
    "\n",
    "There are several options for encoding categorical features.\n",
    "\n",
    "##### Ordinal Encoding\n",
    "\n",
    "In ordinal encoding, in which each categorical value takes on an integer value. The ations of ordinal encoding are that it doesn't add extra columsn in the feature matrix, which can dilute the other features included. The major drawback of an ordinal encoding is that it inserts a notion of a relationship between each category and the dependent variable (price, in this case). In some situations, this may be appropriate (e.g. in the bed_type feature, there is a natural ordering/relationship between the bed_type and the price of the listing.\n",
    "\n",
    "##### One-Hot (Dummy) Encoding\n",
    "\n",
    "In a one-hot encoding scheme, each category is represented as its own new feature that takes on a binary value - 1 if the input fits into a given category. For a category with *n* levels, this means adding *n* new columns to our feature matrix. For low values of *n*, this may be okay, but higher values of *n* risk the possibiilty of blowing the dimensionality of the feature matrix way out of proportion. \n",
    "\n",
    "##### Binary Encoding\n",
    "\n",
    "Binary encoding is a cool alternative to one-hot encoding for the representation of categorical values. First, each value takes on an integer value in an ordinal encoding. From this, each value may be represented as a binary number. Finally, this binary number is split up into individual bits, and each is inserted into the feature matrix as a new column. \n",
    "\n",
    "### Encoding Choices\n",
    "\n",
    "Armed with a wealth of information on categorical encoding, we made the following decisions for each of the categorical features in the dataset.\n",
    "\n",
    "#### Neighbourhood\n",
    "Due to its high dimensionality (420 possibilities), we chose a binary encoding to represent the neighbourhood feature. An ordinal encoding might also be possible, but in research we've done, a binary encoding almost always seemed to ourperform ordinal encodings.\n",
    "\n",
    "#### Property Type\n",
    "Property was another category that had multiple dimensions (26 options), hence we decided to process the categorical data by encoding them with a binary method. We initially attempted sum encoding however that did not yield improved results. We ended up reducing the \"represented\" number of features from 26 to 5. \n",
    "\n",
    "#### Room Type & Bed Type\n",
    "Observing the results from the previous encoding procedures we decided to perform a different form of encoding, where we utilized a combination of one-hot and binary encoding. This method significantly aided our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_ordinal_mapping(data, parameter):\n",
    "    parameter_mapping = {}\n",
    "    \n",
    "    for index, parameter in enumerate(sorted(data[parameter].unique())):\n",
    "        parameter_mapping[parameter] = index\n",
    "    \n",
    "    return parameter_mapping\n",
    "\n",
    "neighbourhood_mapping = create_ordinal_mapping(listingsAll, \"neighbourhood_cleansed\")\n",
    "bed_type_mapping = create_ordinal_mapping(listingsAll, \"bed_type\")\n",
    "room_type_mapping = create_ordinal_mapping(listingsAll, \"room_type\")\n",
    "property_type_mapping = create_ordinal_mapping(listingsAll, \"property_type\")\n",
    "\n",
    "# Sorting based on room quality\n",
    "shared_room_ord = room_type_mapping[\"Shared room\"] \n",
    "entire_apt_ord = room_type_mapping[\"Entire home/apt\"]\n",
    "room_type_mapping[\"Shared room\"] = entire_apt_ord\n",
    "room_type_mapping[\"Entire home/apt\"] = shared_room_ord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper\n",
    "\n",
    "The class `Helper` exposes a set of helpful functions that each deal with processing an individual feature in the dataset. For example, `amenities_to_list` takes in an individual row from the `amenities` column - represented as a string - and converts it into a more useful list format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Helper():\n",
    "    def __init__(self, data_attrs):\n",
    "        self.data_attrs = data_attrs\n",
    "    \n",
    "    # Converts string representation of amenities list \n",
    "    def amenities_to_list(self, amenities):\n",
    "        amenities = amenities.replace('{', '')\n",
    "        amenities = amenities.replace('}', '')\n",
    "        amenities = amenities.replace('\\\"', '')\n",
    "        return amenities.split(',')\n",
    "\n",
    "    # Creates new feature out of the number of amenities\n",
    "    def num_amenities(self, amenitiesList):\n",
    "        return len(amenitiesList)\n",
    "\n",
    "    # Converts string representation of price to float \n",
    "    def price_to_int(self, price):\n",
    "        price = price.replace(',', '')\n",
    "        price = price.replace('$', '')\n",
    "        return float(price)\n",
    "    \n",
    "    def date_to_days(self, startDay):\n",
    "        dStart = datetime.strptime(startDay, \"%Y-%m-%d\")\n",
    "        dEnd = datetime.strptime(self.data_attrs['date_min'], \"%Y-%m-%d\")\n",
    "        return abs((dEnd - dStart).days)\n",
    "    \n",
    "    def row_to_ordinal(self, row, mapping):\n",
    "        return self.data_attrs[mapping][row]\n",
    "\n",
    "    \n",
    "helper = Helper({'date_min': listingsAll[listingsAll.host_since.isnull() == False].host_since.max(), \n",
    "                 'neighbourhood_mapping': neighbourhood_mapping, \n",
    "                 'bed_type_mapping':bed_type_mapping, \n",
    "                 'room_type_mapping':room_type_mapping,\n",
    "                 'property_type_mapping':property_type_mapping\n",
    "                });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Processor\n",
    "The class `cleanProcessor` does handles two things - it applies the methods defined in `Helper` to its dataset and performs some additional processing like null-filling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cleanProcessor():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def clean_listings(self):\n",
    "        df_clean = self.data.copy()\n",
    "        \n",
    "        # TODO: What? - Better techniques for filling nulls\n",
    "        df_clean.loc[df_clean.review_scores_rating.isnull(), 'review_scores_rating'] = 90\n",
    "        df_clean.loc[df_clean.host_since.isnull(), 'host_since'] = '2015-10-02'\n",
    "        df_clean.loc[df_clean.bedrooms.isnull(), 'bedrooms'] = 0\n",
    "        df_clean.loc[df_clean.bathrooms.isnull(), 'bathrooms'] = 0\n",
    "        df_clean.loc[df_clean.beds.isnull(), 'beds'] = 0\n",
    "        df_clean.loc[df_clean.property_type.isnull(), 'property_type'] = \"Other\"\n",
    "        \n",
    "        df_clean['amenities'] = df_clean['amenities'].apply(helper.amenities_to_list)\n",
    "        df_clean['num_amenities'] = df_clean['amenities'].apply(helper.num_amenities)\n",
    "        df_clean['price'] = df_clean['price'].apply(helper.price_to_int)\n",
    "        df_clean['days_host'] = df_clean['host_since'].apply(helper.date_to_days)\n",
    "        df_clean['neighbourhood_binary'] = df_clean['neighbourhood_cleansed'].apply(helper.row_to_ordinal, args=(\"neighbourhood_mapping\",))\n",
    "        df_clean['property_binary_encoded'] = df_clean['property_type'].apply(helper.row_to_ordinal, args=(\"property_type_mapping\",))\n",
    "        \n",
    "        # Binary encoding for neighborhoods\n",
    "        encoder = be.BinaryEncoder(cols=['neighbourhood_binary'])\n",
    "        binary_neighbourhoods = encoder.transform(df_clean)\n",
    "        \n",
    "        # Sum encoding for property type\n",
    "        property_encoder = be.BinaryEncoder(cols=['property_binary_encoded'])\n",
    "        binary_properties = property_encoder.transform(df_clean)\n",
    "        \n",
    "        # One-hot and Ordinal Encoding for bed_type and room_type\n",
    "        bed_type = pd.get_dummies(df_clean.bed_type)    \n",
    "        df_clean[\"bed_type\"] = df_clean[\"bed_type\"].apply(helper.row_to_ordinal, args=(\"bed_type_mapping\",))\n",
    "        print df_clean[\"bed_type\"]\n",
    "        room_type = pd.get_dummies(df_clean.room_type)    \n",
    "        df_clean[\"room_type\"] = df_clean[\"room_type\"].apply(helper.row_to_ordinal, args=(\"room_type_mapping\",))\n",
    "\n",
    "        # One-hot encoding for cancellation policy\n",
    "        cancellation_policy = pd.get_dummies(df_clean.cancellation_policy)\n",
    "        cancellation_policy.rename(columns={'strict':'cancellation_strict'})\n",
    "        cancellation_policy.rename(columns={'flexible':'cancellation_flexible'})\n",
    "        cancellation_policy.rename(columns={'moderate':'cancellation_moderate'})\n",
    "                \n",
    "        data = pd.concat([df_clean, cancellation_policy, binary_neighbourhoods, bed_type, room_type, binary_properties], axis=1)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def clean_calendar(self):\n",
    "        self.df_clean = df.copy()\n",
    "        self.df_clean = self.df_clean[self.df_clean.available == 't']\n",
    "        self.df_clean['price'] = self.df_clean['price'].apply(helper.price_to_int)\n",
    "        return self.df_clean\n",
    "\n",
    "clean_processor_listings = cleanProcessor(listingsAll)\n",
    "clean_processor_calendar = cleanProcessor(calendarAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's use our clean_processor to clean our listings data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       4\n",
      "1       4\n",
      "2       4\n",
      "3       4\n",
      "4       4\n",
      "5       4\n",
      "6       4\n",
      "7       4\n",
      "8       4\n",
      "9       4\n",
      "10      4\n",
      "11      4\n",
      "12      4\n",
      "13      4\n",
      "14      4\n",
      "15      4\n",
      "16      4\n",
      "17      4\n",
      "18      4\n",
      "19      4\n",
      "20      4\n",
      "21      4\n",
      "22      4\n",
      "23      4\n",
      "24      4\n",
      "25      4\n",
      "26      4\n",
      "27      4\n",
      "28      4\n",
      "29      4\n",
      "       ..\n",
      "3788    4\n",
      "3789    4\n",
      "3790    4\n",
      "3791    4\n",
      "3792    4\n",
      "3793    4\n",
      "3794    4\n",
      "3795    4\n",
      "3796    4\n",
      "3797    2\n",
      "3798    4\n",
      "3799    4\n",
      "3800    4\n",
      "3801    4\n",
      "3802    4\n",
      "3803    4\n",
      "3804    4\n",
      "3805    4\n",
      "3806    4\n",
      "3807    4\n",
      "3808    4\n",
      "3809    4\n",
      "3810    4\n",
      "3811    4\n",
      "3812    4\n",
      "3813    4\n",
      "3814    4\n",
      "3815    4\n",
      "3816    4\n",
      "3817    4\n",
      "Name: bed_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "listingsClean = clean_processor_listings.clean_listings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Private -- 80.0 \n",
      "Shared -- 47.0 \n",
      "Entire -- 159.0 \n"
     ]
    }
   ],
   "source": [
    "print '{} -- {} '.format('Private', listingsClean[listingsClean.room_type == 1].price.median()) \n",
    "print '{} -- {} '.format('Shared', listingsClean[listingsClean.room_type == 0].price.median()) \n",
    "print '{} -- {} '.format('Entire', listingsClean[listingsClean.room_type == 2].price.median()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Real Bed': 4, 'Futon': 2, 'Couch': 1, 'Pull-out Sofa': 3, 'Airbed': 0}\n"
     ]
    }
   ],
   "source": [
    "print bed_type_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real -- 120.0 \n",
      "Pull-out -- 79.5 \n",
      "Futon -- 70.0 \n",
      "Air -- 65.0 \n",
      "Couch -- 55.0 \n"
     ]
    }
   ],
   "source": [
    "print '{} -- {} '.format('Real', listingsClean[listingsClean.bed_type == 4].price.median())\n",
    "print '{} -- {} '.format('Pull-out', listingsClean[listingsClean.bed_type == 3].price.median()) \n",
    "print '{} -- {} '.format('Futon', listingsClean[listingsClean.bed_type == 2].price.median()) \n",
    "print '{} -- {} '.format('Air', listingsClean[listingsClean.bed_type == 0].price.median()) \n",
    "print '{} -- {} '.format('Couch', listingsClean[listingsClean.bed_type == 1].price.median()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Models\n",
    "\n",
    "Now that we've done a good amount of initial preprocessing on our data, let's move towards some predictive modeling. Our goal is to develop a model that will predict a price given listing parameters. We'll experiment with including different features, and see which combination fo features and models produces the best results.\n",
    "\n",
    "After some initial research we've decided to start off by trying four different models - \n",
    "\n",
    "1. Linear Lasso\n",
    "       The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent.\n",
    "       \n",
    "2. ElasticNet \n",
    "       ElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\n",
    "       \n",
    "3. Support Vector Regression\n",
    "       Support Vector Regression is an implementation of regression that uses the SVM approach. In this case, we'll be using the linear kernel.\n",
    "            \n",
    "4. Ridge Regression\n",
    "       Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares.\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear Lasso\n",
    "lasso = linear_model.Lasso(alpha = .01)\n",
    "\n",
    "# ElasticNet\n",
    "elasticNet = linear_model.ElasticNet(alpha = 0.1, l1_ratio=0.7)\n",
    "\n",
    "# Ridge Regression\n",
    "ridgeRegression = linear_model.Ridge(alpha = .5)\n",
    "\n",
    "# Support Vector Regression\n",
    "svr = svm.SVR(C=1.0, epsilon=0.2)\n",
    "\n",
    "models = {'lasso': lasso, 'elasticNet': elasticNet, 'ridgeRegression': ridgeRegression }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Models\n",
    "\n",
    "Now, let's define `ModelHelper`, a class that will facilitate the process of testing our models.\n",
    "\n",
    "#### A quick note on testing\n",
    "\n",
    "To test our model, we'll be using the `cross_val_score` method provided by scikit-learn. Additionally, we'll be using `cross_val_score` coupled with a StratifiedKFold train-test splitting step. This step ensures that each neighborhood will have equal representation in both the training and test datasets. This is necessary because the data is sorted by neighbourhood. By default, a train-test split would likely give us training and test sets where a given neighbourhood only appears in one or the other - essentially nullifying the predictive power of the neighbourhood as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelHelper():\n",
    "    ''' \n",
    "    ModelHelper exposes a set of functions aimed at facilitating the dataset\n",
    "    manipulation (train-test splitting) and model testing process\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, X, y, features):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.XFeat = X[features]\n",
    "    \n",
    "    def cross_validate(self, model, cv=3):\n",
    "        '''Cross-validates model within trainnig set with a split of 'cv' - default value of 3'''\n",
    "        cv = StratifiedKFold(self.X.neighbourhood_cleansed)\n",
    "        return cross_val_score(model, self.XFeat, self.y, cv=cv).mean()\n",
    "\n",
    "    def train_test_splitter(self, model, train_size=0.6, save=False):\n",
    "        '''Performs train-test split on data, trains on train, tests on test, returns score, model, data'''\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.XFeat, self.y, train_size=train_size)\n",
    "        model.fit(X_train, y_train)\n",
    "        return X_train, X_test, y_train, y_test, model\n",
    "\n",
    "    def test_models(self, models):\n",
    "        '''Iterates over all different models and print out their results of train_test_splitter'''\n",
    "        for modelName, model in models.iteritems():\n",
    "            print '{} : {}'.format(modelName, self.cross_validate(model))\n",
    "    \n",
    "    def save_model(self, fitted_model, filename=\"model.pkl\"):\n",
    "        '''Persists model to disk for later use in backend API'''\n",
    "        joblib.dump(fitted_model, filename) \n",
    "\n",
    "    def save_mappings(self, filename=\"mappings.pkl\"):\n",
    "        '''Persists model to disk for later use in backend API'''\n",
    "        property_type_mapping_copy = property_type_mapping.copy()\n",
    "        room_type_mapping_copy = room_type_mapping.copy()\n",
    "        bed_type_mapping.update(property_type_mapping_copy)\n",
    "        bed_type_mapping.update(room_type_mapping_copy)        \n",
    "        bed_type_mapping.update(neighbourhood_mapping)\n",
    "        joblib.dump(bed_type_mapping, filename) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "binary_encoded_properties = [\"property_binary_encoded_{}\".format(i) for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['num_amenities', 'bedrooms', 'beds', 'neighbourhood_binary_0', 'neighbourhood_binary_1', 'neighbourhood_binary_2', 'neighbourhood_binary_3', 'neighbourhood_binary_4', 'neighbourhood_binary_5', 'neighbourhood_binary_6', 'neighbourhood_binary_7', 'neighbourhood_binary_8', 'bathrooms', 'accommodates', 'bed_type', 'Airbed', 'Couch', 'Futon', 'Pull-out Sofa', 'Real Bed', 'room_type', 'Entire home/apt', 'Private room', 'Shared room', 'property_binary_encoded_0', 'property_binary_encoded_1', 'property_binary_encoded_2', 'property_binary_encoded_3', 'property_binary_encoded_4']\n"
     ]
    }
   ],
   "source": [
    "# TODO: add property_type to features --> encoding\n",
    "features = ['bedrooms', 'beds',\n",
    "            'neighbourhood_binary_0', 'neighbourhood_binary_1', \n",
    "            'neighbourhood_binary_2', 'neighbourhood_binary_3',\n",
    "            'neighbourhood_binary_4', 'neighbourhood_binary_5',\n",
    "            'neighbourhood_binary_6', 'neighbourhood_binary_7',\n",
    "            'neighbourhood_binary_8', 'bathrooms', 'accommodates',\n",
    "            'bed_type', 'Airbed', 'Couch', 'Futon', 'Pull-out Sofa',\n",
    "            'Real Bed', 'room_type', 'Entire home/apt', 'Private room',\n",
    "            'Shared room']\n",
    "\n",
    "allFeatures = features + binary_encoded_properties\n",
    "\n",
    "print allFeatures\n",
    "\n",
    "pickle.dump(allFeatures, open( '../app/utils/modelFeatures.pkl', 'wb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticNet : 0.275481132136\n",
      "ridgeRegression : 0.276254078323\n",
      "lasso : 0.276260692505\n"
     ]
    }
   ],
   "source": [
    "mHelper = ModelHelper(listingsClean, listingsClean.price, allFeatures)\n",
    "\n",
    "mHelper.test_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save dict mappings\n",
    "mHelper.save_mappings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
