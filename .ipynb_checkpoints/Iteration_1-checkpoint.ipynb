{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airbnb Price Prediction - Optimizing Listings\n",
    "#### Patrick Huston & Filippos Lymperopoulos | Spring 2016\n",
    "\n",
    "*This notebook aims to document our process in creating a ML pipeline to predict Airbnb listing prices given an input set of features. A major focus of this exploration is to write modular, well-designed components that could easily be taken and applied to a different modeling situation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "0. What should we do with space?\n",
    "1. Explore encoding for bed_type\n",
    "2. Explore encoding for room_type\n",
    "3. Explore encoding for property_type\n",
    "4. Explore encoding for amenities\n",
    "5. Explore log-loss + understand accuracy meaning (what is .28? is that good?)\n",
    "6. Document our code and decisions we've made\n",
    "    - CleanProcessor: filling nulls, binary encoding, representing features\n",
    "    - Explain our decisions in feature engineering and why we did them\n",
    "    - Modeling decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import binaryHelper as be\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "Airbnb provides an expansive listings dataset for public use. It includes all listings for major cities around the world. For the purpose of our exploration, we'll start off by using well-known US cities - Boston, San Francisco, Los Angeles, Washington DC, and Seattle. For each listing, Airbnb provides a large amount of features - check [here](https://github.com/flymperopoulos/DataScience16CTW/blob/master/report/listings_features.md) to see them all listed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Listings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listingsBoston = pd.read_csv('./data/listingsBoston.csv')\n",
    "listingsSF = pd.read_csv('./data/listingsSF.csv')\n",
    "listingsLA = pd.read_csv('./data/listingsLA.csv')\n",
    "listingsDC = pd.read_csv('./data/listingsDC.csv')\n",
    "listingsSeattle = pd.read_csv('./data/listingsSeattle.csv')\n",
    "\n",
    "frames = [listingsBoston, listingsSF, listingsLA, listingsDC, listingsSeattle]\n",
    "\n",
    "listingsAll = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Calendar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calendarBoston = pd.read_csv('./data/calendarBoston.csv')\n",
    "calendarSF = pd.read_csv('./data/calendarSF.csv')\n",
    "calendarLA = pd.read_csv('./data/calendarLA.csv')\n",
    "calendarDC = pd.read_csv('./data/calendarDC.csv')\n",
    "calendarSeattle = pd.read_csv('./data/calendarSeattle.csv')\n",
    "\n",
    "frames = [calendarBoston, calendarSF, calendarLA, calendarDC, calendarSeattle]\n",
    "\n",
    "calendarAll = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Convenience\n",
    "\n",
    "To facilitate the process of cleaning, we've defined a cleaning helper and cleaning processor class below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_neighbourhood_mapping(data):\n",
    "    neighbourhood_mapping = {}\n",
    "    \n",
    "    for index, neighbourhood in enumerate(sorted(data.neighbourhood_cleansed.unique())):\n",
    "        neighbourhood_mapping[neighbourhood] = index\n",
    "    \n",
    "    return neighbourhood_mapping\n",
    "\n",
    "neighbourhood_mapping = create_neighbourhood_mapping(listingsAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper\n",
    "\n",
    "The class `Helper` exposes a set of helpful functions that each deal with processing an individual feature in the dataset. For example, `amenities_to_list` takes in an individual row from the `amenities` column - represented as a string - and converts it into a more useful list format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Helper():\n",
    "    def __init__(self, data_attrs):\n",
    "        self.data_attrs = data_attrs\n",
    "    \n",
    "    # Converts string representation of amenities list \n",
    "    def amenities_to_list(self, amenities):\n",
    "        amenities = amenities.replace('{', '')\n",
    "        amenities = amenities.replace('}', '')\n",
    "        amenities = amenities.replace('\\\"', '')\n",
    "        return amenities.split(',')\n",
    "\n",
    "    # Creates new feature out of the number of amenities\n",
    "    def num_amenities(self, amenitiesList):\n",
    "        return len(amenitiesList)\n",
    "\n",
    "    # Converts string representation of price to float \n",
    "    def price_to_int(self, price):\n",
    "        price = price.replace(',', '')\n",
    "        price = price.replace('$', '')\n",
    "        return float(price)\n",
    "    \n",
    "    def date_to_days(self, startDay):\n",
    "        dStart = datetime.strptime(startDay, \"%Y-%m-%d\")\n",
    "        dEnd = datetime.strptime(self.data_attrs['date_min'], \"%Y-%m-%d\")\n",
    "        \n",
    "        return abs((dEnd - dStart).days)\n",
    "    \n",
    "    def neighbourhood_to_ordinal(self, neighbourhood):\n",
    "        return self.data_attrs['neighbourhood_mapping'][neighbourhood]\n",
    "    \n",
    "helper = Helper({'date_min': listingsAll[listingsAll.host_since.isnull() == False].host_since.max(), 'neighbourhood_mapping': neighbourhood_mapping});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Processor\n",
    "The class `cleanProcessor` does handles two things - it applies the methods defined in `Helper` to its dataset and performs some additional processing like null-filling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cleanProcessor():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def clean_listings(self):\n",
    "        df_clean = self.data.copy()\n",
    "        \n",
    "        # TODO: What? - Better techniques for filling nulls\n",
    "        df_clean.loc[df_clean.review_scores_rating.isnull(), 'review_scores_rating'] = 90\n",
    "        df_clean.loc[df_clean.host_since.isnull(), 'host_since'] = '2015-10-02'\n",
    "        df_clean.loc[df_clean.bedrooms.isnull(), 'bedrooms'] = 0\n",
    "        df_clean.loc[df_clean.bathrooms.isnull(), 'bathrooms'] = 0\n",
    "        df_clean.loc[df_clean.beds.isnull(), 'beds'] = 0\n",
    "        \n",
    "        df_clean['amenities'] = df_clean['amenities'].apply(helper.amenities_to_list)\n",
    "        df_clean['num_amenities'] = df_clean['amenities'].apply(helper.num_amenities)\n",
    "        df_clean['price'] = df_clean['price'].apply(helper.price_to_int)\n",
    "        df_clean['days_host'] = df_clean['host_since'].apply(helper.date_to_days)\n",
    "        df_clean['neighbourhood_binary'] = df_clean['neighbourhood_cleansed'].apply(helper.neighbourhood_to_ordinal)\n",
    "        \n",
    "        \n",
    "        # Binary encoding for neighborhoods\n",
    "        encoder = be.BinaryEncoder(cols=['neighbourhood_binary'])\n",
    "        binary_neighbourhoods = encoder.transform(df_clean)\n",
    "        \n",
    "        # One-hot encoding for cancellation policy\n",
    "        cancellation_policy = pd.get_dummies(df_clean.cancellation_policy)\n",
    "        cancellation_policy.rename(columns={'strict':'cancellation_strict'})\n",
    "        cancellation_policy.rename(columns={'flexible':'cancellation_flexible'})\n",
    "        cancellation_policy.rename(columns={'moderate':'cancellation_moderate'})\n",
    "        \n",
    "        data = pd.concat([df_clean, cancellation_policy, binary_neighbourhoods], axis=1)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def clean_calendar(self):\n",
    "        self.df_clean = df.copy()\n",
    "        self.df_clean = self.df_clean[self.df_clean.available == 't']\n",
    "        self.df_clean['price'] = self.df_clean['price'].apply(helper.price_to_int)\n",
    "        return self.df_clean\n",
    "\n",
    "clean_processor_listings = cleanProcessor(listingsAll)\n",
    "clean_processor_calendar = cleanProcessor(calendarAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Real Bed' 'Futon' 'Airbed' 'Pull-out Sofa' 'Couch']\n",
      "['House' 'Apartment' 'Villa' 'Condominium' 'Other' 'Bed & Breakfast' 'Loft'\n",
      " 'Townhouse' 'Boat' 'Castle' 'Dorm' nan 'Bungalow' 'Cabin' 'Tent' 'Island'\n",
      " 'Camper/RV' 'Plane' 'Yurt' 'Treehouse' 'Chalet' 'Hut' 'Earth House'\n",
      " 'Lighthouse' 'Parking Space' 'Tipi' 'Train']\n",
      "id                                  36751\n",
      "listing_url                         36751\n",
      "scrape_id                           36751\n",
      "last_scraped                        36751\n",
      "name                                36751\n",
      "summary                             34787\n",
      "space                               28941\n",
      "description                         36737\n",
      "experiences_offered                 36751\n",
      "neighborhood_overview               23753\n",
      "notes                               17754\n",
      "transit                             23952\n",
      "thumbnail_url                       33554\n",
      "medium_url                          33554\n",
      "picture_url                         36751\n",
      "xl_picture_url                      33554\n",
      "host_id                             36751\n",
      "host_url                            36751\n",
      "host_name                           36744\n",
      "host_since                          36751\n",
      "host_location                       36679\n",
      "host_about                          26558\n",
      "host_response_time                  32500\n",
      "host_response_rate                  32500\n",
      "host_acceptance_rate                30284\n",
      "host_is_superhost                   36744\n",
      "host_thumbnail_url                  36744\n",
      "host_picture_url                    36744\n",
      "host_neighbourhood                  33021\n",
      "host_listings_count                 36744\n",
      "                                    ...  \n",
      "review_scores_communication         27444\n",
      "review_scores_location              27394\n",
      "review_scores_value                 27392\n",
      "requires_license                    36751\n",
      "license                               657\n",
      "jurisdiction_names                  15210\n",
      "instant_bookable                    36751\n",
      "cancellation_policy                 36751\n",
      "require_guest_profile_picture       36751\n",
      "require_guest_phone_verification    36751\n",
      "calculated_host_listings_count      36751\n",
      "reviews_per_month                   27916\n",
      "num_amenities                       36751\n",
      "days_host                           36751\n",
      "neighbourhood_binary                36751\n",
      "flexible                            36751\n",
      "moderate                            36751\n",
      "no_refunds                          36751\n",
      "strict                              36751\n",
      "super_strict_30                     36751\n",
      "super_strict_60                     36751\n",
      "neighbourhood_binary_0              36751\n",
      "neighbourhood_binary_1              36751\n",
      "neighbourhood_binary_2              36751\n",
      "neighbourhood_binary_3              36751\n",
      "neighbourhood_binary_4              36751\n",
      "neighbourhood_binary_5              36751\n",
      "neighbourhood_binary_6              36751\n",
      "neighbourhood_binary_7              36751\n",
      "neighbourhood_binary_8              36751\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "listingsClean = clean_processor_listings.clean_listings()\n",
    "\n",
    "print listingsClean.bed_type.unique()\n",
    "print listingsClean.property_type.unique()\n",
    "\n",
    "print listingsClean[listingsClean.square_feet.isnull()].count()\n",
    "# print listingsClean.amenities.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Models\n",
    "\n",
    "Now that we've done a good amount of initial preprocessing on our data, let's move towards some predictive modeling. Our goal is to develop a model that will predict a price given listing parameters. We'll experiment with including different features, and see which combination fo features and models produces the best results.\n",
    "\n",
    "After some initial research we've decided to start off by trying four different models - \n",
    "\n",
    "1. Linear Lasso\n",
    "       The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent.\n",
    "       \n",
    "2. ElasticNet \n",
    "       ElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\n",
    "       \n",
    "3. Support Vector Regression\n",
    "       Support Vector Regression is an implementation of regression that uses the SVM approach. In this case, we'll be using the linear kernel.\n",
    "            \n",
    "4. Ridge Regression\n",
    "       Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares.\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear Lasso\n",
    "lasso = linear_model.Lasso(alpha = 0.1)\n",
    "\n",
    "# ElasticNet\n",
    "elasticNet = linear_model.ElasticNet(alpha = 0.1, l1_ratio=0.7)\n",
    "\n",
    "# Ridge Regression\n",
    "ridgeRegression = linear_model.Ridge(alpha = .5)\n",
    "\n",
    "# Support Vector Regression\n",
    "svr = svm.SVR(C=1.0, epsilon=0.2)\n",
    "\n",
    "models = {'lasso': lasso, 'elasticNet': elasticNet, 'ridgeRegression': ridgeRegression }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Models\n",
    "\n",
    "Now, let's define `ModelHelper`, a class that will facilitate the process of testing our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelHelper():\n",
    "    ''' \n",
    "    ModelHelper exposes a set of functions aimed at facilitating the dataset\n",
    "    manipulation (train-test splitting) and model testing process\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def cross_validate(self, model, cv=3):\n",
    "        '''Cross-validates model within trainnig set with a split of 'cv' - default value of 3'''\n",
    "        return cross_validation.cross_val_score(model, self.X, self.y, cv=cv).mean()\n",
    "\n",
    "    def train_test_splitter(self, model, train_size=0.5):\n",
    "        '''Performs train-test split on data, trains on train, tests on test, returns score, model, data'''\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, train_size=train_size)\n",
    "        model.fit(X_train, y_train)\n",
    "        return X_train, X_test, y_train, y_test, model\n",
    "\n",
    "    def test_models(self, models):\n",
    "        '''Iterates over all different models and print out their results of train_test_splitter'''\n",
    "        for modelName, model in models.iteritems():\n",
    "            X_train, X_test, y_train, y_test, model = self.train_test_splitter(model, train_size=0.5)\n",
    "            print '{} : {}'.format(modelName, model.score(X_test, y_test))\n",
    "\n",
    "    def test_model(self, model):\n",
    "        '''Test one specific model with train_test_splitter'''\n",
    "        X_train, X_test, y_train, y_test, model = self.train_test_splitter(model, train_size=0.5)\n",
    "        print model.score(X_test, y_test)\n",
    "        return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticNet : 0.280106570013\n",
      "ridgeRegression : 0.290496970365\n",
      "lasso : 0.301523040026\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define list of features\n",
    "\n",
    "# print listingsClean[listingsClean.beds.isnull()].count()\n",
    "\n",
    "features = ['days_host', 'num_amenities', 'bedrooms', 'beds',\n",
    "            'neighbourhood_binary_0', 'neighbourhood_binary_1', \n",
    "            'neighbourhood_binary_2', 'neighbourhood_binary_3',\n",
    "            'neighbourhood_binary_4', 'neighbourhood_binary_5',\n",
    "            'neighbourhood_binary_6', 'neighbourhood_binary_7',\n",
    "            'neighbourhood_binary_8', 'bathrooms', 'accommodates']\n",
    "\n",
    "mHelper = ModelHelper(listingsClean[features], listingsClean.price)\n",
    "\n",
    "mHelper.test_models(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
