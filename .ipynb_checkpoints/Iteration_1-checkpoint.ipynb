{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Airbnb Price Prediction - Optimizing Listings\n",
    "#### Patrick Huston & Filippos Lymperopoulos | Spring 2016\n",
    "\n",
    "*This notebook aims to document our process in creating a ML pipeline to predict Airbnb listing prices given an input set of features. A major focus of this exploration is to write modular, well-designed components that could easily be taken and applied to a different modeling situation.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "0. What should we do with space?\n",
    "1. Explore encoding for bed_type\n",
    "2. Explore encoding for room_type\n",
    "3. Explore encoding for property_type\n",
    "4. Explore encoding for amenities\n",
    "5. Explore log-loss + understand accuracy meaning (what is .28? is that good?)\n",
    "6. Document our code and decisions we've made\n",
    "    - CleanProcessor: filling nulls, binary encoding, representing features\n",
    "    - Explain our decisions in feature engineering and why we did them\n",
    "    - Modeling decisions\n",
    "7. Nulls in room_type? Get them from the space description\n",
    "8. Is there anything interesting in the name of the listing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import binaryHelper as be\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data\n",
    "\n",
    "Airbnb provides an expansive listings dataset for public use. It includes all listings for major cities around the world. For the purpose of our exploration, we'll start off by using well-known US cities - Boston, San Francisco, Los Angeles, Washington DC, and Seattle. For each listing, Airbnb provides a large amount of features - check [here](https://github.com/flymperopoulos/DataScience16CTW/blob/master/report/listings_features.md) to see them all listed out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Listings Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "listingsBoston = pd.read_csv('./data/listingsBoston.csv')\n",
    "listingsSF = pd.read_csv('./data/listingsSF.csv')\n",
    "listingsLA = pd.read_csv('./data/listingsLA.csv')\n",
    "listingsDC = pd.read_csv('./data/listingsDC.csv')\n",
    "listingsSeattle = pd.read_csv('./data/listingsSeattle.csv')\n",
    "\n",
    "frames = [listingsBoston, listingsSF, listingsLA, listingsDC, listingsSeattle]\n",
    "\n",
    "listingsAll = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Calendar Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calendarBoston = pd.read_csv('./data/calendarBoston.csv')\n",
    "calendarSF = pd.read_csv('./data/calendarSF.csv')\n",
    "calendarLA = pd.read_csv('./data/calendarLA.csv')\n",
    "calendarDC = pd.read_csv('./data/calendarDC.csv')\n",
    "calendarSeattle = pd.read_csv('./data/calendarSeattle.csv')\n",
    "\n",
    "frames = [calendarBoston, calendarSF, calendarLA, calendarDC, calendarSeattle]\n",
    "\n",
    "calendarAll = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Convenience\n",
    "\n",
    "To facilitate the process of cleaning, we've defined a cleaning helper and cleaning processor class below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_neighbourhood_mapping(data):\n",
    "    neighbourhood_mapping = {}\n",
    "    \n",
    "    for index, neighbourhood in enumerate(sorted(data.neighbourhood_cleansed.unique())):\n",
    "        neighbourhood_mapping[neighbourhood] = index\n",
    "    \n",
    "    return neighbourhood_mapping\n",
    "\n",
    "neighbourhood_mapping = create_neighbourhood_mapping(listingsAll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper\n",
    "\n",
    "The class `Helper` exposes a set of helpful functions that each deal with processing an individual feature in the dataset. For example, `amenities_to_list` takes in an individual row from the `amenities` column - represented as a string - and converts it into a more useful list format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Helper():\n",
    "    def __init__(self, data_attrs):\n",
    "        self.data_attrs = data_attrs\n",
    "    \n",
    "    # Converts string representation of amenities list \n",
    "    def amenities_to_list(self, amenities):\n",
    "        amenities = amenities.replace('{', '')\n",
    "        amenities = amenities.replace('}', '')\n",
    "        amenities = amenities.replace('\\\"', '')\n",
    "        return amenities.split(',')\n",
    "\n",
    "    # Creates new feature out of the number of amenities\n",
    "    def num_amenities(self, amenitiesList):\n",
    "        return len(amenitiesList)\n",
    "\n",
    "    # Converts string representation of price to float \n",
    "    def price_to_int(self, price):\n",
    "        price = price.replace(',', '')\n",
    "        price = price.replace('$', '')\n",
    "        return float(price)\n",
    "    \n",
    "    def date_to_days(self, startDay):\n",
    "        dStart = datetime.strptime(startDay, \"%Y-%m-%d\")\n",
    "        dEnd = datetime.strptime(self.data_attrs['date_min'], \"%Y-%m-%d\")\n",
    "        \n",
    "        return abs((dEnd - dStart).days)\n",
    "    \n",
    "    def neighbourhood_to_ordinal(self, neighbourhood):\n",
    "        return self.data_attrs['neighbourhood_mapping'][neighbourhood]\n",
    "    \n",
    "helper = Helper({'date_min': listingsAll[listingsAll.host_since.isnull() == False].host_since.max(), 'neighbourhood_mapping': neighbourhood_mapping});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Processor\n",
    "The class `cleanProcessor` does handles two things - it applies the methods defined in `Helper` to its dataset and performs some additional processing like null-filling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class cleanProcessor():\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def clean_listings(self):\n",
    "        df_clean = self.data.copy()\n",
    "        \n",
    "        # TODO: What? - Better techniques for filling nulls\n",
    "        df_clean.loc[df_clean.review_scores_rating.isnull(), 'review_scores_rating'] = 90\n",
    "        df_clean.loc[df_clean.host_since.isnull(), 'host_since'] = '2015-10-02'\n",
    "        df_clean.loc[df_clean.bedrooms.isnull(), 'bedrooms'] = 0\n",
    "        df_clean.loc[df_clean.bathrooms.isnull(), 'bathrooms'] = 0\n",
    "        df_clean.loc[df_clean.beds.isnull(), 'beds'] = 0\n",
    "        \n",
    "        df_clean['amenities'] = df_clean['amenities'].apply(helper.amenities_to_list)\n",
    "        df_clean['num_amenities'] = df_clean['amenities'].apply(helper.num_amenities)\n",
    "        df_clean['price'] = df_clean['price'].apply(helper.price_to_int)\n",
    "        df_clean['days_host'] = df_clean['host_since'].apply(helper.date_to_days)\n",
    "        df_clean['neighbourhood_binary'] = df_clean['neighbourhood_cleansed'].apply(helper.neighbourhood_to_ordinal)\n",
    "        \n",
    "        \n",
    "        # Binary encoding for neighborhoods\n",
    "        encoder = be.BinaryEncoder(cols=['neighbourhood_binary'])\n",
    "        binary_neighbourhoods = encoder.transform(df_clean)\n",
    "        \n",
    "        # One-hot encoding for cancellation policy\n",
    "        cancellation_policy = pd.get_dummies(df_clean.cancellation_policy)\n",
    "        cancellation_policy.rename(columns={'strict':'cancellation_strict'})\n",
    "        cancellation_policy.rename(columns={'flexible':'cancellation_flexible'})\n",
    "        cancellation_policy.rename(columns={'moderate':'cancellation_moderate'})\n",
    "        \n",
    "        data = pd.concat([df_clean, cancellation_policy, binary_neighbourhoods], axis=1)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def clean_calendar(self):\n",
    "        self.df_clean = df.copy()\n",
    "        self.df_clean = self.df_clean[self.df_clean.available == 't']\n",
    "        self.df_clean['price'] = self.df_clean['price'].apply(helper.price_to_int)\n",
    "        return self.df_clean\n",
    "\n",
    "clean_processor_listings = cleanProcessor(listingsAll)\n",
    "clean_processor_calendar = cleanProcessor(calendarAll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Real Bed' 'Futon' 'Airbed' 'Pull-out Sofa' 'Couch']\n",
      "['House' 'Apartment' 'Villa' 'Condominium' 'Other' 'Bed & Breakfast' 'Loft'\n",
      " 'Townhouse' 'Boat' 'Castle' 'Dorm' nan 'Bungalow' 'Cabin' 'Tent' 'Island'\n",
      " 'Camper/RV' 'Plane' 'Yurt' 'Treehouse' 'Chalet' 'Hut' 'Earth House'\n",
      " 'Lighthouse' 'Parking Space' 'Tipi' 'Train']\n",
      "['Private room' 'Entire home/apt' 'Shared room']\n"
     ]
    }
   ],
   "source": [
    "listingsClean = clean_processor_listings.clean_listings()\n",
    "\n",
    "print listingsClean.bed_type.unique()\n",
    "print listingsClean.property_type.unique()\n",
    "print listingsClean.room_type.unique()\n",
    "\n",
    "# print listingsClean[listingsClean.bed_type.isnull()].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Private -- 96.3242164829 \n",
      "Shared -- 62.3242134063 \n",
      "Entire -- 228.545401889 \n"
     ]
    }
   ],
   "source": [
    "print '{} -- {} '.format('Private', listingsClean[listingsClean.room_type == 'Private room'].price.median()) \n",
    "print '{} -- {} '.format('Shared', listingsClean[listingsClean.room_type == 'Shared room'].price.median()) \n",
    "print '{} -- {} '.format('Entire', listingsClean[listingsClean.room_type == 'Entire home/apt'].price.median()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real -- 120.0 \n",
      "Futon -- 70.0 \n",
      "Air -- 65.0 \n",
      "Pull-out -- 79.5 \n",
      "Couch -- 55.0 \n"
     ]
    }
   ],
   "source": [
    "print '{} -- {} '.format('Real', listingsClean[listingsClean.bed_type == 'Real Bed'].price.median())\n",
    "print '{} -- {} '.format('Pull-out', listingsClean[listingsClean.bed_type == 'Pull-out Sofa'].price.median()) \n",
    "print '{} -- {} '.format('Futon', listingsClean[listingsClean.bed_type == 'Futon'].price.median()) \n",
    "print '{} -- {} '.format('Air', listingsClean[listingsClean.bed_type == 'Airbed'].price.median()) \n",
    "print '{} -- {} '.format('Couch', listingsClean[listingsClean.bed_type == 'Couch'].price.median()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Models\n",
    "\n",
    "Now that we've done a good amount of initial preprocessing on our data, let's move towards some predictive modeling. Our goal is to develop a model that will predict a price given listing parameters. We'll experiment with including different features, and see which combination fo features and models produces the best results.\n",
    "\n",
    "After some initial research we've decided to start off by trying four different models - \n",
    "\n",
    "1. Linear Lasso\n",
    "       The Lasso is a linear model that estimates sparse coefficients. It is useful in some contexts due to its tendency to prefer solutions with fewer parameter values, effectively reducing the number of variables upon which the given solution is dependent.\n",
    "       \n",
    "2. ElasticNet \n",
    "       ElasticNet is a linear regression model trained with L1 and L2 prior as regularizer. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\n",
    "       \n",
    "3. Support Vector Regression\n",
    "       Support Vector Regression is an implementation of regression that uses the SVM approach. In this case, we'll be using the linear kernel.\n",
    "            \n",
    "4. Ridge Regression\n",
    "       Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of coefficients. The ridge coefficients minimize a penalized residual sum of squares.\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear Lasso\n",
    "lasso = linear_model.Lasso(alpha = 0.1)\n",
    "\n",
    "# ElasticNet\n",
    "elasticNet = linear_model.ElasticNet(alpha = 0.1, l1_ratio=0.7)\n",
    "\n",
    "# Ridge Regression\n",
    "ridgeRegression = linear_model.Ridge(alpha = .5)\n",
    "\n",
    "# Support Vector Regression\n",
    "svr = svm.SVR(C=1.0, epsilon=0.2)\n",
    "\n",
    "models = {'lasso': lasso, 'elasticNet': elasticNet, 'ridgeRegression': ridgeRegression }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Models\n",
    "\n",
    "Now, let's define `ModelHelper`, a class that will facilitate the process of testing our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ModelHelper():\n",
    "    ''' \n",
    "    ModelHelper exposes a set of functions aimed at facilitating the dataset\n",
    "    manipulation (train-test splitting) and model testing process\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def cross_validate(self, model, cv=3):\n",
    "        '''Cross-validates model within trainnig set with a split of 'cv' - default value of 3'''\n",
    "        return cross_validation.cross_val_score(model, self.X, self.y, cv=cv).mean()\n",
    "\n",
    "    def train_test_splitter(self, model, train_size=0.5):\n",
    "        '''Performs train-test split on data, trains on train, tests on test, returns score, model, data'''\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, train_size=train_size)\n",
    "        model.fit(X_train, y_train)\n",
    "        return X_train, X_test, y_train, y_test, model\n",
    "\n",
    "    def test_models(self, models):\n",
    "        '''Iterates over all different models and print out their results of train_test_splitter'''\n",
    "        for modelName, model in models.iteritems():\n",
    "            X_train, X_test, y_train, y_test, model = self.train_test_splitter(model, train_size=0.5)\n",
    "            print '{} : {}'.format(modelName, model.score(X_test, y_test))\n",
    "\n",
    "    def test_model(self, model):\n",
    "        '''Test one specific model with train_test_splitter'''\n",
    "        X_train, X_test, y_train, y_test, model = self.train_test_splitter(model, train_size=0.5)\n",
    "        print model.score(X_test, y_test)\n",
    "        return model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticNet : 0.280106570013\n",
      "ridgeRegression : 0.290496970365\n",
      "lasso : 0.301523040026\n"
     ]
    }
   ],
   "source": [
    "# TODO: Define list of features\n",
    "\n",
    "# print listingsClean[listingsClean.beds.isnull()].count()\n",
    "\n",
    "features = ['days_host', 'num_amenities', 'bedrooms', 'beds',\n",
    "            'neighbourhood_binary_0', 'neighbourhood_binary_1', \n",
    "            'neighbourhood_binary_2', 'neighbourhood_binary_3',\n",
    "            'neighbourhood_binary_4', 'neighbourhood_binary_5',\n",
    "            'neighbourhood_binary_6', 'neighbourhood_binary_7',\n",
    "            'neighbourhood_binary_8', 'bathrooms', 'accommodates']\n",
    "\n",
    "mHelper = ModelHelper(listingsClean[features], listingsClean.price)\n",
    "\n",
    "mHelper.test_models(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
